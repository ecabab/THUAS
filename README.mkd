# Applied Data Science Portfolio
*Emilio Caba Batuecas - Dwelling Energy Insights*

My name is Emilio Caba, and this portfolio is a representation of all that I have learned and accomplished while taking the Applied Data Science minor in The Hague University of Applied Sciences, placed in The Hague (The Netherlands).

The main goal of the course was to be able to carry out a real data science project in teams of six people. With the purpose to accomplish this goal, I have taken several lectures and online courses in DataCamp. The project where I have participated was called _Dwelling Energy Insights_ and its development is explained in this portfolio.

The team members were Santiago Puertas Puchol, Olivier van Luijk, Teo Čurčić, Merel Kreszner, Rajeev Kalloe and me, and the product owner was Dr. T.B. Salcedo Rahola.


## Table of Contents
1. [Research project](#research-project)
	- 1.1 [Task definition](#task-definition)
	- 1.2 [Evaluation](#evaluation)
	- 1.3 [Conclusions](#conclusions)
	- 1.4 [Planning](#planning)

2. [Domain knowledge](#domain-knowledge)
	- 2.1 [Introduction of the subject field](#introduction-of-the-subject-field)
	- 2.2 [Literature research](#literature-research)
	- 2.3 [Explanation of Terminology, jargon and definitions](#explanation-of-terminology-jargon-and-definitions)
	
3. [Data preprocessing](#data-preprocessing)
	- 3.1 [Data exploration](#data-exploration)
	- 3.2 [Data cleansing](#data-cleansing)
	- 3.3 [Data preparation](#data-preparation)
	- 3.4 [Data explanation](#data-explanation)
	- 3.5 [Data visualization](#data-visualization)
	
4. [Predictive Analytics](#predictive-analytics)
	- 4.1 [Selecting a Model](#selecting-a-model)
	- 4.2 [Configuring a Model](#configuring-a-model)
	- 4.3 [Training a model](#training-a-model)
	- 4.3 [Evaluating a model](#evaluating-a-model)
	- 4.3 [Visualizing the outcome of a model](#visualizing-the-outcome-of-a-model)

5. [Communication](#communication)
	- 5.1 [Presentations](#presentations)
	- 5.2 [Writing paper](#writing-paper)

### DataCamp courses
During the Applied Data Science minor, I have been asked to complete some DataCamp courses as a complement to enrich our knowledge about data science. All of them were completed as requested on time as shown below:

![Image of Assignments](https://github.com/ecabab/THUAS/blob/master/Images/DataCamp_Assignments.png)

Apart from those mandatory courses, I have also started some career tracks as I find them really interesting. These are the following:

* **Python Programmer (52 hours)**: 9 out of 13 courses completed.
* **Data Analyst with Python (60 hours)**: 8 out of 16 courses completed.
* **Data Scientist with Python (100 hours)**: 13 out of 26 courses completed.
* **Machine Learning Scientist with Python (93 hours)**: 11 out of 23 courses completed.

![Image of Tracks](https://github.com/ecabab/THUAS/blob/master/Images/DataCamp_CareerTracks.png)

### Reflection and evaluation

#### Own contribution

From September 2019 to January 2020, I took an Applied Data Science minor at The Hague University of Applied Sciences where I participated in the development of a research project called _Dwelling Energy Insights_ as part of a team of six people.<br>
The main goal of this project was to get as much insight as possible from 33 dwellings of a neighborhood called _Groene Mient_. Specifically, we wanted to be able to cluster the dwellings and predict the following features on each dwelling: the heating system installed, the number of inhabitants living there and the number of solar panels.<br>
The way to achieve this goal was by analyzing the data that comes from the smart meters which are installed on these dwellings. These smart meters measure the energy delivery and energy consumption of each dwelling every 15 minutes. 

One of the first indispensable tasks in every data science project is to clean the raw data. As I had previously taken a course about data cleaning, I started working on it while the rest of the group were taking some courses about it on _DataCamp_. Once they finished learning, I shared with them the knowledge I had about data cleaning and the progress so far that I had done. The main task was to get as much insight as possible from the smart meters data by applying _machine learning_ algorithms.<br>
Apart from the data cleaning process, each one of us chose a different clustering algorithm to implement on the cleaned data. In my case, I chose _K-Nearest Neighbours_.</br>
At the end of the project, we had to write a research paper which was divided into different parts.

In order to get some domain knowledge about this topic, I did some research about smart meters and energy in The Netherlands by reading research papers and similar projects.<br>
For the data cleaning process, I applied the methods I previously learned which are the following: looking for outliers, looking for blank spaces, filling blank spaces if possible, and delete the wrong data. I also added data from other resources (weather data from _KNMI_) and created dummy variables.<br>
Apart from the dataset from _Groene Mient_, a second dataset of 120 dwellings was received, this one was not labeled, but also needed to be cleaned. Therefore, I cleaned and prepare it to be used for the models.</br>
With the purpose of getting insight from the dwellings and compare them, I created a table that shows when and how much each dwelling had consumed the most, so we could check if there was any pattern or relation between them or their characteristics.</br>
Apart from that, I applied the _[TSNE and K-Means](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%20TSNE%20and%20K-Means.ipynb)_ with my partner Santiago Puertas Puchol with the purpose of checking if it would give us more insight.</br>
Once the data was cleaned, I applied the _K-Nearest Neighbours_ classifier by using different configuration parameters to find out which ones fitted the best. I created three models, one of them for each target feature (heating system, number of inhabitants and number of solar panels).</br>
Apart from this approach, I also tried to classify the samples by if they had installed one heating system or not, so the different models will return if its heating system was _E_ or not, _WP_ or not and, _Zon_ or not. This apporach wa implemented by _LR_, _SVM_ and _KNN_.
Once we got all the results, I wrote the following parts of the research paper: Data Preprocessing, K-Nearest Neighbours implementation and the final conclusion.

I did a good data cleaning process and we use the data from _Groene Mient_ to feed all the different models and algorithms that we tried out. The second dataset was also properly cleaned.<br>
The consumption table was suscessfully created and use afterwards to get more insight about these dwellings.<br>
From the _TSNE_ and _K-Means_, it was not possible to get much insight a I implemented it very early in the project so I did not have enough knowledge about it.
About the _K-Nearest Neighbours_, I implemented it by using different configuration parameters, but it did not give me enough good results even though they were better than the _Logistic Regression_ and the _Support Vector Machine_ results (63% for heating system, 41% for nº of inhabitants, and 54% for nº of solar panels). However, it did not overcome the accuracy of the _LSTM_ model (95% for heating system, 87% for nº of inhabitants, and 80% for nº of solar panels).
About the approach of predicting if a dwelling had installed one type or heating system or not, the model which performed the best was the _KNN_, with an accuracy of 85% for _Zon_, 74% for _WP_ and 72% for _E_.

When cleaning the data, I learned to analyze and extract the usable data from a big dataset. I used different methods with the purpose of applying the best for this case and acquired new cleaning skills.<br>
Furthermore, I learned how to build, configure, train and evaluate a model by applying the _K-Nearest Neighbours, Logistic Regression and Support Vector Machine_, which can be applicable for any other model. Apart from that, now I am able to analyze and compare different models and choose which one fits better on each case. During this whole process, I learned how to configure the hyperparameters by using a Pipeline and how to evaluate the model by using cross-validation among others.
Doing some research and writing the paper provided me and improved some research skills that I have never used before.


#### Own learning

From September 2019 to January 2020, I took an Applied Data Science minor at The Hague University of Applied Sciences where I participated in the development of a research project called _Dwelling Energy Insights_ as part of a team of six people.<br>
The purpose of this minor was to learn and apply the basics of data science to a real project in The Netherlands, apart from researching and explaining the results in a research paper.

The main goal of the course was to learn as much as possible about data science. The courses consisted of the following subtopics: machine learning (exploratory data analysis, data, validation, and evaluation), research and neuronal networks.<br>
As I worked on a real project, I also had to learn how to work in a group and with our product owner, hence I had to learn some teamwork skills like _SCRUM_ and communicative skills.

With the objective to learn as much as possible, I had different ways to get the knowledge.
I had classes every Monday where the teacher explained to me different topics about data science, they also provide me with tutorial notebooks in which I could go over the concepts and other notebooks to practice. <br>
Besides the lectures, I was also subscribed to DataCamp. I had to take some mandatory courses with the aim of learning the basics. Apart from those, I also took some other courses to learn more about data science.<br>
While I was learning, I applied all the new concepts to the project I was working on and also contrasting them with the research projects that were previously done on this topic.

The combination of different knowledge resources resulted in great learning, which allowed me to get all the basics in the different fields of data science but also to get deeper knowledge in the topic of our project and the teamwork and communicative skills.
By now, I am able to apply the following machine learning algorithms: _Linear Regression, Logistic Regression, Support Vector Machine, K-Nearest Neighbours, K-Means and the LSTM RNN_.<br>
Apart from that, I also learned how to research in official documents, take advantage of the work that is already done, apply it to our project and write a research paper based on the new development and results that we have accomplished.

At the beginning of the course, I just was able to do some cleaning on the data based on different courses that I previously took. By taking this minor I have learned how to carry out a complete exploratory data analysis, apply and compare different algorithms to the data and extract good results from the whole development of the project.<br>
Besides the technical knowledge, I have also learned how to deal with my colleges, split the tasks and share the knowledge between us by following the SCRUM methodology.</br>
I am also proud of being able to research and write a research paper, which was something that I thought was really difficult.


#### Group project

From September 2019 to January 2020, I took an Applied Data Science minor at The Hague University of Applied Sciences where I participated in the development of a research project called _Dwelling Energy Insights_ as part of a team of six people.<br>
The main goal of this project was to get as much insight as possible from 33 dwellings of a neighborhood called _Groene Mient_, specifically, we wanted to be able to cluster the dwellings and predict the following features on each dwelling: the heating system installed, the number of inhabitants and the number of solar panels.<br>
The way to achieve this goal was by analyzing the data that comes from the smart meters which are installed on these dwellings. These smart meters measure the energy delivery and energy consumption of each dwelling every 15 minutes. 

In order to achieve this goal, we had to analyze and clean the data received from the smart pumps, try out different algorithms, evaluate and compare them and take out the best results. As a group, we applied the _SCRUM_ methodology by splitting and assigning the tasks to different people.<br>
The tasks that we did are the following: We all did some research on the topic and share with each other the content of the papers read; we also cleaned the data by using different approaches. About the machine learning, _Logistic Regression_ and _K-Means_ were implemented by Teo, the _Support Vector Machine_ was implemented by Merel and Rajeev, I implemented the _K-Nearest Neighbours_ and Santiago and Olivier implemented the _LSTM RNN_.
We all together wrote the research paper and presented the improvements of the project development in class every week.

We all together cleaned the data by using different approaches (deleting all the data from some dwellings, deleting blank spaces, filling the blank spaces…). We started clustering the data by using _TSNE_ and _PCA_, but we did not get good results, so we started implementing the clustering algorithms mentioned above.<br>
From the K-means we could not get any information about what was happening on the clustering. Then we started with _LR_, _SVM_, and _KNN_, we set different configuration params on each algorithm in order to get the best results, we could get some results but not good enough to set them as final results. Finally, we implemented the _LSTM RNN_, which gave us the best results, it fitted the best because this kind of _RNN_ takes care of the timestamp and learns through the time.

The results that we got when we wanted to predict the heating system, number of inhabitants and number of solar panels are the following: _Logistic Regression_: 52%, 38%, and 46%; _Support Vector Machine_: 63%, 42%, and 56%; _K-Nearest Neighbours_: 63%, 41%, and 54%; _LSTM_: 96%, 88%, and 81%. As we can see, the _LSTM RNN_ gave us the best results.<br>
In general, we all did good work on the project by helping each other and solving the challenges set at each moment. We were also able to write a research paper that includes the whole development of the project.

When developing the whole project, I learned to carry out a complete exploratory data analysis, apply and compare different algorithms to the data and extract good results from the whole development of the project, including the writing of the research paper.<br>
Besides the technical knowledge, I also learned how to deal with my colleges, split the tasks and share the knowledge between us by following the _SCRUM_ methodology.


### Project Development (_Dwelling Energy Insights_)
* #### Research Project
  ##### Task definition
  
	This research focuses on dwellings which are part of the built environment to gain insights into the energy delivery and consumption because it is critical to classify the characteristics of dwellings that do not use natural gasses. The reason why this is so critical is, because with information about the dwelling, it will be easier to decide which heating system and how many solar panels are needed for a dwelling to comply with the NZEB standard. 

	With the smart meter data, it should be possible to **predict what kind of heating system is used, how many solar panels are installed and how many inhabitants are living in this dwelling.** This paper focusses on **comparing the accuracy of models created by different machine learning algorithms**, which can classify the different characteristics based on smart meter data and KNMI weather data. 

  ##### Evaluation

  	With the purpose of achieving this goal, we will develop a complete study about the dwellings with the information gathered by the smart meters and the weather data which comes from the _KNMI_.
  	
  	Once we get some results by applying _Machine Learning_ to the data in order to know if it is possible to get insight about these dwellings from the available data, we will compare the results from the different algorithms that can fit for our purpose and we will end with some conclusions.
  
  ##### Conclusion

  	In this part of the chapter, the results of the relevant algorithms used are discussed.  

	By using Logistic Regression (LR), Support Vector Machine (SVM) and K-Nearest Neighbours (KNN) algorithms, the best prediction results that we got were 63% of accuracy for the heating system, 42% of accuracy for the number of inhabitants and 56% of accuracy for the number of solar panels. So, when presenting one register of the dataset, the algorithms can tell us about 63% for sure which heating system is used by that dwelling. This means is it only partly possible to gain insight into the characteristics of the dwellings using the data from smart meters. 

	We decided to use Recurrent Neural Network (RNN), specifically the Long short-term memory (LSTM) one, to see if results are any different. Here we managed to get an accuracy of 96% for the heating system, 88% for the number of inhabitants and 81% for the number of solar panels. With the LSTM algorithm, we can find the answer to the characteristics of the dwelling like what kind of heating system is used, the number of inhabitants living in the dwelling and the number of solar panels installed in that dwelling. Considered that companies search for a minimum of 95% accuracy, we can now answer the research question about the heating system type installed in the dwellings, for the other characteristics, there is still work to be done.

	To test the outcomes of the model, an excel file had been created showing the number of successes and errors on the data using the LSTM. For the test, we chose randomly some dwellings that fit the different characteristics, for those dwellings, we took the first two days of each month and the results are shown in the table below, where green cells mean that the prediction is correct whereas the red ones mean that the prediction is incorrect.
	
	![Image of Testing](https://github.com/ecabab/THUAS/blob/master/Images/Testing_LSTM.png)
  
  ##### Planning

  	During the project development, I have used an agile approach using SCRUM. For doing so we created an online environment in Trello where we have been setting the different tasks which had to be done. We divided the whole development in sprints of two weeks and every day we had a daily stand-up to discuss what was done and what will be worked on that day.
  	
  	
  	The Trello dashboard consisted of 4 different lists (_To Do, In Progress, Done and To Refine_). At the end of every sprint, a new backlog is defined and added to the list _To Refine_. At the beginning of the next sprint, a meeting decides what will be worked on during the sprint and moved to the list _To Do_. The tasks in the _To Do_ list will be assigned to certain persons in the team. When working on it the task will be moved to the list _In Progress_ and when done to the list _Done_.
  	
  	![Image of Testing](https://github.com/ecabab/THUAS/blob/master/Images/Trello_dashboard.png)
  	
* #### Domain Knowledge
  * **Introduction of the subject field**

  	In the last few years, the European Commission has set various ambitious targets in order to reduce CO2 emissions. As a result of this, the Dutch government has been regulating the energy sector to move towards the use of green energy in order to avoid the use of natural gasses completely by the end of 2050. Moreover, in the Netherlands, there has been a growing concern with the use of natural gas, because of the exponential growth of earthquakes caused by the extraction of gas from the Groningen gas field. Accordingly, the urgency of reducing the use of natural gasses in dwellings is growing strongly. Although most energy in the Netherlands is used by industry, the built environment is responsible for 28% of the energy use from which 71% is for heating purposes. Currently, 87% of the buildings heating energy needs are covered with the use of natural gas. The technologies that currently exist which could help us reduce the use of natural gasses are easier to implement in the built environment rather than the industry since the industry often has higher temperature needs than the built environment. 

	To reduce the use of gas in the built environment, some new and renovated dwellings have already been constructed to Net-Zero Energy Building (NZEB) standard. This means that for these dwellings the total amount of energy used on an annual basis is equal to the amount of renewable energy created on-site. The type of energy produced and used by this type of dwellings in the Netherlands is only electrical. The electricity energy use and production patterns of NZEB dwellings differ a lot from the average dwellings, because of the variety of heating and energy-producing systems used.  

	Since these dwellings have such different energy patterns, new approaches are needed to be able to take technical/political decisions about what type of technologies to promote. Government-organizations can, therefore, use this information to create/improve (existing) policies. To set up these policies, information about dwelling energy usage and delivery can be collected from the dwellings. A good technique to collect that information is via smart meters. 

	By 2020, 80% of the households in The Netherlands will have a smart meter. A smart meter measures the amount of energy taken from and sent to the net. This data, in combination with publicly available data, could allow determining dwelling characteristics and the energy use behavior of its residents. This information could be used to optimize the current dwellings energy systems, gain more knowledge about energy use patterns and to propose possible improvements with the goal to reduce CO2 emissions related to energy use.   

	Previous research shows insights about household electricity consumption and CO2 emissions on dwellings in the Netherlands. It explores the effect of the smart meter, appliance efficiency and consumer behavior on reducing electricity consumption in the Netherlands. The results show their effect on electricity consumption and suggest that further effort is required to control and reduce it. Insights from the paper suggest that future studies should disaggregate with respect to several factors in electricity consumption as George Papachristos has stated in his paper.

	Based on the research of Papachristos, behavioral patterns have been formed based on the characteristics of dwellings and electricity consumption. It analyses the appliance uses in the Dutch housing stock and define behavioral patterns and profiles of electricity consumption in detail. This has been done with survey data that were collected from 323 dwellings in the Netherlands on appliance ownership and the use of electricity. 

	Currently, smart meters are being installed in most of the Dutch dwellings. Data from the smart meters contains important information about the minimum and maximum amount of energy circling back and forward in the net. This is needed to change the current net infrastructure because the current infrastructure is not built for receiving as much energy from dwellings. The energy produced at dwellings should be able to be sent back to the net without little to no energy loss. This energy can then be used at moments without sun and/or wind. 

	A thorough analysis of 15-minute residential smart meter datasets, it is possible to identify possible value propositions of smart meter measurements. The results showed that for different applications, the communication needs from meters to control-centers, data storage capabilities, and the complexity of data processing intelligence varies significantly. 

	When introducing the people in the dwelling with their consumer behavior, this can be used to change their behavior and therefore reduce the yearly energy cost. The consumption behavior is based on the amount of energy every device in the dwelling is using, visualized by logistic regression machine learning. Another research proves that the use of machine learning algorithms can be used to forecast residential gas consumption based on energy consumption data and weather data. Gaining insight into the energy consumption with machine learning algorithms can be helpful in balancing the grid and insights in how to reduce energy consumption can be received. 

	This research focuses on gaining insights on the dwelling characteristics by using machine learning algorithms on smart meter data in a combination with weather data. Specifically, the paper compares the accuracy of the models classifying the dwellings by type of heating system, the number of solar panels and the number of inhabitants.
	
  * **Literature research**

  	During the first week of our project, I started by exploring the domain by doing literature research (see 1.4.1). I have looked at previously published papers that were available at well-known platforms such as ScienceDirect and TU Delft.  
	I looked at researches that were previously done on the subject field, as well as researches that were done on smart meters. I did this to get more insights into previously researched topics that could have helped us to get more understanding of our subject field.   
	 
	We have found several papers that were on the topic of energy usage in dwellings, such as: 
	
	* H. Tommerup, J. Rose, S. Svendsen, Energy-efficient houses built according to the energy performance requirements introduced in Denmark in 2006, (2006).
	
	* George Papachristos, Household electricity consumption and CO2 emissions in the Netherlands: A model-based analysis, (2014).
	
	* Georgios Syngrosa, Constantinos A. Balaras, Dimitrios G. Koubogiannis, Embodied CO2 Emissions in Building Construction Materials of Hellenic Dwellings, (2017).
	
	* D. Majcen, L.C.M. Itard, H. Visscher, Theoretical vs. actual energy consumption of labeled dwellings in the Netherlands: Discrepancies and policy implications, (2012).
	
	Next to that, we have also found a paper on smart meters (Pol Van Aubel & Erik Poll, Smart metering in the Netherlands: What, how, and why, (2019)). This paper helped us to get more understanding of smart meters.
  
  * **Explanation of terminology, jargon and definitions**

  	**Smart meter**
  	
  	Our dataset contains energy delivery and consumption data from smart meters, which are installed in every dwelling in the _Groene Mient_. A smart meter is a digital gas and electricity meter, which can be monitored constantly. They have replaced the analog gas and electricity meter.   

	Smart meters measure the current flow and voltage periodically. Adding these numbers will give the used power. The gas consumption is measured similarly. An In-Home Display (IHD) is connected to the meters and can be used to show your current power and gas consumption and how much it costs. This information can be used to reduce electricity and gas consumption.
	
	**Acronyms**
	
	* Machine learning algorithms:
		* LG = Logistic Regression
		* SVM = Support Vector Machine
		* KNN = K-Nearest Neighbour

	* Heating system types:
		* E = Electricity
		* ZON = Thermal solar panels
		* WP = Heat Pump
  
* #### Data Preprocessing
  * **Data exploration:**
    During the development of this project, I will be using data from three different resources:
	  * _**Groene Mient**_: The first dataset belongs to Groene Mient, a neighborhood placed on The Hague (The Netherlands) which consists of 33 houses. This dataset is labeled and will be used to create the different models which allow us to make the predictions.
	  * _**Anonymous neighborhood**_: The second dataset belongs to a neighborhood placed on The Netherlands which consists of 120 houses. This dataset is not labeled, I will use it to feed our models with different data and see how they behave.
	  * _**KNMI**_: In order to get more insight into the characteristics of the dwelling, I will use weather data to improve our models, this data comes from the *Koninklijk Nederlands Meteorologisch Instituut*.

	    #### _**Groene Mient**_
	    The data from _Groene Mient_ comes inside an excel file which consists of two different sheets:
	    
		* **_Productie_ sheet**: It contains the energy delivery read by the smart meter.
		* **_Consumptie_ sheet**: It contains the energy consumption read by the smart meter.
		
		Both of the sheets contain some other information about the dwellings that I will explain later.
		
	[EDA - Data Exploration of _Groene Mient_ dataset](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%201.%20Data%20Exploration.ipynb)
		: This notebook contains the data exploration process that has been followed to take the first look at the data. It consists of the following steps:
		
	1. I have deleted redundant information and splitted the data into three different datasets: *df_deli*, *df_cons* and *df_info*, which have been exported for the next steps.
	2. I have found outliers in the data that I will have to handle.
		
	The next step is to clean the dataset.
    
    
  * **Data cleaning:** Once I explained the data that I am going to use, I will start cleaning it.

	[EDA - Data Cleaning](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%202.%20Data%20Cleaning.ipynb)
	: This notebook contains the data cleaning process that has been followed to keep the usable data. It consists of the following steps:
		
	1. I checked whether a record is an outlier or not by using the following requisites:
		*  **Energy Delivery**: These solar panels are able to produce a maximum of 250 watts/hour (0.25 kwh) each one. Since the smart meter sums up all the energy delivery every 15 minutes, I will compute what is the maximum production for each dwelling taking into account the number of solar panels. Then, every value over the maximum will be set as an outlier.
		*  **Energy Consumption:** The maximum consumption that any dwelling can put up with is measured by the following equation, where the kWh is converted to W (0.001) in an interval of 15 minutes (0.25h), where the voltage that the dwelling can handle is 230V and 75A.
	2. I set those outliers to NaN values in order to know the total amount of useless data on each dwelling.
	3. I deleted those dwellings which have quite less data (NaN and outliers) than the other ones ('H05', 'H10', 'H11', 'H14', 'H27', 'H30').
	4. I deleted those records which have at least one NaN value to be able to compare every dwelling.
  
  * **Data preparation:** After cleaning the datasets, I will prepare the data to be inserted in the models.
  
  	[EDA - Data Preparation](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%203.%20Data%20Preparation.ipynb)
  : This notebook contains the data preparation process that has been followed to set the data that is going to be inserted on the different models. It consists of the following steps:
  
  	1. I will add the weather data from the _KNMI_ to the energy datasets.
  	2. I will add _dummy variables_ (_hour, day of the week, day of the month, week of the month, month and season_) to the energy datasets.
  	3. I will compact the data by hour, day and week in order to check which one will fit better the models.
  	4. I will place the data from one dwelling below each other as required by the models.

  	After cleaning the data, I also created a [table](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%203.1.%20Amperes%20Table.ipynb) which shows the maximum energy delivery and consumption (15-min data), and the maximum current (daily data) for each dwelling. The purpose of this task was to get more insight from the dwellings.

  	[EDA - Data Preparation for LSTM](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%204.%20Data%20Preparation%20for%20LSTM.ipynb)
  : This notebook contains the data preparation process that has been followed to set the data that is going to be inserted on the LSTM RNN. This notebook follows a different process because every dataset needs to be balanced.
  
  * **Data explanation:** The data that I am going to use to feed the models consists of the combination of the energy delivery and consumption, the weather data from the _KNMI_ and the _dummy variables_ that have created.

  	Apart from that, as said in the previous section, the dataset is indexed by its timestamp and the interval is 15 minutes. I have created three different datasets to check which of them fits the best with the models. These are the following:
  
  	* 15-min dataset: This dataset has intervals of 15 minutes.
  	* Hourly dataset: This dataset has intervals of 1 hour.
  	* Daily dataset: This dataset has intervals of 1 day.
  	* Weekly dataset: This dataset has intervals of 1 week.

  	In order to create these new datasets, I have had to modify the data as explained below:
  
  	* The energy delivery and consumption have been summed up.
  	* The data from the _KNMI_ has been replaced by the mean of the interval.
  	* The dummy variables have been replaced by the mode of the interval.
  
  
  * **Data visualization:** Once I explained the data, I will visualize it in order to get a better insight into the process that we have followed from the raw data until the data that I will use in the models.

  	[EDA - Data Visualization](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%205.%20Data%20Visualization.ipynb)
  : This notebook contains the data visualization process that has been followed to understand better the EDA (Exploratory Data Analysis) process done before. It consists of the following steps:
  
  	* Visualization of the raw data.
  	* Visualization of the cleaned data.
  	* Visualization of the weather data from the _KNMI_.
  	* Visualization of the correlation matrix of the full dataset.

  #### Anonymous neighborhood

  In the second dataset (Anonymous neighborhood), every dwelling has its own excel file which a sheet called _Smart Meter_. The only thing that differs from the process that I followed on the previous dataset is that I read the _Smart Meter_ sheet of every dwelling and I created a new dataset that contained the _smart meter_ data of every dwelling. As I said before, this dataset was not labeled but the product owner asked for the number of solar panels installed on each dwelling. We never received this data so we were not able to test our models with this dataset.
  
  * [EDA - Data Exploration](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%206.%20Data%20Exploration%20(Anonymous%20dataset).ipynb): This notebook contains the data exploration process, where I read the _Smart Meter_ sheet of every dwelling and created one dataset which contained all of them.

  * [EDA - Data Cleaning](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%207.%20Data%20Cleaning%20(Anonymous%20dataset).ipynb): This notebook contains the data cleaning proccess, where I applied the same methods used for the _Groene Mient_ dataset.

  * [EDA - Data Preparation](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%208.%20Data%20Preparation%20(Anonymous%20dataset).ipynb): This notebook contains the data preparation proccess, where the data has been transformed and reshaped to be inserted in the models.

  * [EDA - Data Visualization](https://github.com/ecabab/THUAS/blob/master/EDA/Emilio%20Caba%20-%20EDA%20-%209.%20Data%20Visualization%20(Anonymous%20dataset).ipynb): This notebook contains the data visualization proccess, where I plotted the energy delivery and consumption from some dwellings before and after the cleaning proccess in order to compare them.
  
* #### Predictive Analytics

	Once the data has been cleaned and prepared to be inserted in the models, it is time to create the models. As the main goal is to classify the dwelling by their heating system, the number of inhabitants and number of solar panels, the following supervised classification algorithms are going to be implemented:
	* **[PA - Logistic Regression](https://github.com/ecabab/THUAS/blob/master/PA/Emilio%20Caba%20-%20PA%20-%201.%20Logistic%20Regression.ipynb):** This algorithm was implemented by my teammates, but I tried it with different configuration parameters to get better results by using a Pipeline.
	* **[PA - Support Vector Machine](https://github.com/ecabab/THUAS/blob/master/PA/Emilio%20Caba%20-%20PA%20-%202.%20Support%20Vector%20Machine.ipynb):** This algorithm was implemented by my teammates, but I tried it with different configuration parameters to get better results by using a Pipeline.
	* **[PA - K-Nearest Neighbors](https://github.com/ecabab/THUAS/blob/master/PA/Emilio%20Caba%20-%20PA%20-%203.%20K-Nearest%20Neighbor.ipynb):** I implemented the whole algorithm with different configuration parameters by using a Pipeline.
	* **LSTM:** I did not implement this algorithm, so I just show its results to compare them with the other models.
	* **[PA - Different approach](https://github.com/ecabab/THUAS/blob/master/PA/Emilio%20Caba%20-%20PA%20-%205.%20Different%20approach.ipynb):** This notebook contains the implementation of the LR, SVM, and KNN in order to predict the heating system. This approach consists of determining if a dwelling has a specific heating system type installed or not.

	Every algorithm has been implemented with the purpose of comparing its performance and computing time.

	As said before, I created four different datasets based on the time interval (15-min, hourly, daily and weekly). I have tried all of them and the best results come when I use **weekly data** for LG, SVM and KNN; and **15-min data** for the LSTM, so I will just show the models built based on this approach.
	
	I will predict the **heating system type**, the **amount of inhabitants** who lives in the dwelling and the **amount of solar panels** which are installed on the dwelling.
	
	##### Selecting a model
	
	As I previously said, I selected the **_K-Nearest Negihbors_** algorithm since it is a classification algorithm. In order for this model to work the data must have a notion of distance. In laymens terms, I need to be able to graph our data.
	
	This algorithm is non-linear as _Logistic Regression_ or _Support Vector Machine_ and can be used in classes. Since the data is able to be graphed, as shown before when the TSNE was applied, the _K-Nearest Negihbors_ algorithm will work properly with it.
	
	Once the data is able to be graphed, the _K-Nearest Negihbors_ algorithm will predict the target variable depending on the _n_ samples that surround it. This model will prove useful because, in essence, it looks at the neighboring data points to determine what this new data point will fall into.
	
	##### Configuring a model
	
	With the purpose of creating different models with different configurations, I use a Pipeline, where I specify how to normalize the data, the algorithm to be used and its characteristics.

	In order to normalize the data, I use the StandarScaler library, which standardizes features by removing the mean and scaling to unit variance. The standard score of a sample _x_ is calculated as: _z = (x - u) / s_, where _u_ is the mean of the training samples, and s is the standard deviation of the training samples.

	In this case, I use _K-Nearest Neighbors_. One of the features of the pipeline is that I am able to select different values for the model parameters. The pipeline will execute all the different possibilities and give the best one. For this model, I set the following parameters:
	
	* **n_neighbors:** Number of neighbors to use. For this case, I use the following _n_ values: 5, 6, 7, 8, 9, 10, 11, 12, 13 and 14.
	* **p:** Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan distance (l1), and Euclidean distance (l2) for p = 2. 
	* **weights:** Weight function used in prediction. Possible values:
    	* uniform: uniform weights. All points in each neighborhood are weighted equally.
    	* distance: weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
	
	##### Training a model
	
	With the purpose of building and training the models, I use _GridSearchCV_. This library allows creating different models by setting different values for each parameter and is able to implement the cross-validation. The goal of cross-validation is to define a data set to test the model in the training phase in order to limit problems like overfitting or underfitting and get an insight on how the model will generalize to an independent data set.
	
	Once the models have tried the different possible configurations, it will return just the best one for each target variable.
	
	##### Evaluating a model
	
	These are the results obtained from the training process, as I have used a _GridSearchCV_, only the best models will be shown:
	
	* Heating system: This model accomplished an accuracy of 63% by using the following configuration parameters:
		* _n\_neighbors_: 9
		* _p_: 1
		* _weights_: _uniform_

	* Nº of inhabitants: This model accomplished an accuracy of 41% by using the following configuration parameters:
		* _n\_neighbors_: 13
		* _p_: 1
		* _weights_: _distance_

	* Nº of solar panels: This model accomplished an accuracy of 53% by using the following configuration parameters:
		* _n\_neighbors_: 7
		* _p_: 2
		* _weights_: _distance_
	
	##### Visualizing the outcome of the model
	
	Up to now that I have built and evaluated the models, I am going to predict the different target values of a test set and compare the results with the true values by using the confusion matrix.
	
	![KNN Outcome](https://github.com/ecabab/THUAS/blob/master/Images/KNN_Outcome.png)
	
	As the confusion matrices shows, the model which predicts the heating system works the best when predicting _WP_, _E_ and _Zon_ get worse results because of the number of samples of each heating system; the model which predicts the nº of inhabitants works the best when predicting dwellings with 2 and 4 inhabitants and, the model which predicts the nº of solar panels works the best when predicting those dwellings that have installed between 10 and 14 solar panels.
	
	##### Global results
	
	These are the global results of every model that we have applied to the data:
	
	![Image of Results](https://github.com/ecabab/THUAS/blob/master/Images/Final_Results.png)
	
	As we can see, the model which got the best results is the LSTM RNN, this is because this neuronal network takes into account the previous results while it is learning. Furthermore, it also relates the outputs with the timestamp, which is crucial in the project since the data goes through time.
  
* #### Communication
  * **Presentations**

  	During the whole development of the project, we have given several presentations with the purpose of explaining the last improvements that we have made, but also to get feedback and advice about how to continue working on the project. The table below shows all the presentation we have given during the course:
  	
  	![Image of Presentations](https://github.com/ecabab/THUAS/blob/master/Images/Presentations.png)
  	
  	We have given several types of presentations:
  	
  	* **Closed presentations:** These presentations were given to our teachers and classmates every Monday. Their main goal was to present the progress that we have made so far during the week and get some feedback.

  	* **Open presentations:** These presentations were given to our teachers, classmates and everyone who wanted to assist. Their main goal was to let people know about we were working on.

  	* **TSNE presentation:** This presentation was given to our teachers and classmates in order to explain to them what is the TSNE (t-distributed Stochastic Neighbor Embedding) and how to apply it.
  	
  	* **Groene Mient presentation:** This presentation was given in _Groene Mient_ to the people who live there. The data we have used comes from the dwellings which are placed in this neighborhood and we went there with the aim of explaining everything about the project development.

  	* **TU Delft presentation:** This presentation was given in the _TU Delft_ to some teachers, researches, and people who wanted to assist. The main goal was to get some feedback from the people specialized in energy.

  	From all of these presentations, I gave three closed presentations and the one which took place in _Groene Mient_.
  	
  	* [Closed presentation - Week 7](https://github.com/ecabab/THUAS/blob/master/Presentations/Dwelling%20Energy%20Insights%20-%20week%207.pdf)
  	* [Groene Mient presentation](https://github.com/ecabab/THUAS/blob/master/Presentations/Groene%20Mient.pdf)
  	* [Closed presentation - Week 10](https://github.com/ecabab/THUAS/blob/master/Presentations/Dwelling%20Energy%20Insights%20-%20week%2010.pdf)
  	* [Closed presentation - Week 12](https://github.com/ecabab/THUAS/blob/master/Presentations/Dwelling%20Energy%20Insights%20-%20week%2012.pdf)

  
  * **Research paper**

	Writing a research paper has the purpose of explaining its whole development to those people who are interested in this topic, from the research question until the results and conclusion. It has been the first time writing a research paper for me, so it has been a really good opportunity to learn how to do it.
	
	As I did some research in order to learn how to write it and our product owner has been guiding us during this process by giving us feedback about what we were writing, it has been pretty straight forward. We first created the structure of the paper and then started writing. We assigned the different parts of the paper to all the people of the team, so we all participated. In my case, I explained the _cleaning data process_, the _K-Nearest Neighbours_ implementation, and the final conclusion.
